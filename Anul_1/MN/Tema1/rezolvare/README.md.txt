Implementare Tema 1: Metode Numerice Matriceale
Micu Francesca-Maria 313CB

În rezolvarea celor trei task-uri am folosit implementat funcții:

Taskul 1: Markov is coming:
parse_labyrinth.m:
	Se deschide fișierul primit ca parametru cu funcția fopen si permisiuni de citire(r) de text(t) și se verifică funcționalitatea acestei operații. Pe prima linie a fișierului se află numărul de linii și de coloane a matricei care se rețin în variabila lin_col citită cu funcția fscanf. În n se memorează numărul de linii și în m numărul de coloane. Prin cele două for-uri se citește fișierul linie cu line (datele liniei fiind reținute în variabila linie) și se atribuie fiecărui element din linia matricei Labyrinth respective elementul corespunzător din linia fișierului citit. Pentru fiecare linie i, se citesc m valori întregi din fișier și se stochează în variabila linie. Apoi, fiecare element j din această linie este atribuit poziției corespunzătoare din matricea Labyrinth. La final, se închide fișierul.

get_adjacency_matrix.m:
	Se rețin în n și m numărul de linii ale matricei inițiale. Se calculează apoi numărul total de stări ca fiind n*m+2, unde cele două stări suplimentare reprezintă starea de câștig/pierdere (st_win/st_lose). Matricea Adj se inițializează ca matrice rară (sparse) de dimensiuni n*m+2, cum este cerut în restricții, majoritatea elementelor urmând a fi zero. Funcția parcurge fiecare element din labirint folosind două bucle imbricate și calculează indexul unic în matricea de adiacență pentru element ca fiind idx = (i - 1)*m + j. Pentru acestea, se verifică existența pereților în cele patru direcții cardinale folosind operația bitand (dacă bitul de pe poziția specifică punctului cardinal e 0). Astfel, dacă nu există perete in sus/jos (Nord/Sud) se verifică dacă se ieșirea e localizată pe prima/ultima linie caz în care se actualizează linia/coloana de câștig, dacă nu se actualizează elementul de jos/sus din matricea de adiacență, găsindu-se un nou poisibil drum în labirint. Similar se procedează pentru stânga/dreapta (vest/est), doar că se verifică dacă suntem pe ultima/prima coloană, aici actualizâdu-se starea de pierdere respectivă. La finalul funcției, se adaugă bucle pentru stările de câștig și pierdere. Am utilizat funcția bitand despre care m-am documentat de aici https://book.huihoo.com/gnu-octave-manual-version-3/octave_42.html?utm_source
 
get_link_matrix.m:
	În matricea Adj se reține rezultatul funcției get_adjacency_matrix. Se rețin în n și m numărul de linii, respectiv numărul de coloane și se inițializează matricea Link ca matrice rară folosind funcția specifică sparse. La parcurgerea matricei Adj se determină numărul de elemente egale cu 1 (dacă există legături) din linia respectivă a matricei și se reține în variabila count_legat. După se verifică dacă count_legat a fost incrementat în parcurgerea anterioară și în acest caz se parcurge iar linia și se pune in matricea Linx valoarea rezultată la împărțirea la 1 a numărului de legături acolo unde exista una în matricea de adicență, în caz negativ rămânând 0.

get_Jacobi_parameters.m:
	Se reține în n dimensiunile matricei Link fiind matrice pătratică. Dimensiunea pentru determinarea matricei și vectorului de iterație(n_new) se determină din scăderea primei dimensiuni a celor două linii/coloane corespunzătoare stărilor de win și lose. Se inițializeaza matricea de iterație(G) cu n_new linii și coloane, iar vectorul(c) cu n_new linii și o coloană. Cele două sunt tot matrice rare deci se declara cu funcția sparse. Matricea G primește primele n_new linii și coloane din matricea de legături și vectorul c primește coloana corespunzătoare stării win(n_new + 1).

perform_iterative.m:
	Se determină dimensiunea vectorului c și se stochează în variabila n. Se inițializează vectorul soluție x ca un vector coloană de zerouri cu n linii, contorul de pași steps cu valoarea 1, apoi se atribuie vectorul inițial x0 variabilei x. Funcția parcurge un număr maxim de iterații definit de parametrul max_steps, efectuând în fiecare pas calculul formulei iterative x = G·x0 + c pentru a găsi o nouă aproximare a soluției. După fiecare calcul, se verifică criteriul de convergență comparând norma diferenței dintre noua aproximare x și aproximarea anterioară x0 cu toleranța tol. Dacă această diferență este mai mică decât toleranța specificată, funcția se termină. În caz contrar, se actualizează eroarea err cu valoarea normei diferenței dintre x și x0, se actualizează x0 cu noua aproximare x pentru următoarea iterație și se incrementează contorul de pași. Funcția implementată are la bază funcția Jacobi predată la curs și la laborator, modificată cu formula din secțiunea de teorie 1.3.3.

heuristic_greedy.m:
	Se determină dimensiunea matricei de adiacență n linii/coloane. Se inițializează vectorul path care va conține drumul parcurs, începând cu poziția de start, și se setează nr_path = 1 pentru a indica numărul de elemente din drum. Se creează un vector viz pentru a marca stările vizitate, cu poziția de start marcată ca vizitată. Funcția intră într-o buclă care continuă până când se ajunge la starea de câștig sau până când nu mai poate înainta. În fiecare iterație, se verifică dacă poziția curentă este starea de câștig (n-1), în acest caz, returnându-se drumul găsit. Altfel, se identifică toți vecinii poziției curente folosind matricea de adiacență, verificând conexiunile existente. Dintre acești vecini, se selectează doar cei nevizitați. Dacă nu există vecini nevizitați, algoritamul se întoarce, decrementând nr_path și continuând cu poziția anterioară. Dacă există vecini nevizitați, se folosește vectorul probabilities pentru a selecta vecinul nevizitat cu cea mai mare probabilitate de câștig. Acest vecin este marcat ca vizitat, adăugat la drum și devine noua poziție curentă. La finalul algoritmului, se returnează drumul complet de la poziția de start până la starea de câștig. Algoritmul reprezintă implementarea pseudocodului din secțiunea 1.3.4 de teorie.

decode_path.m:
	Se determină lungimea vectorului path (și se reține în n) și se verifică dacă ultima poziție din drum este starea de câștig (lines*cols + 1). În cazul în care drumul se termină cu starea de câștig, aceasta este eliminată din vector, neavând o corespondență în labirint. După această verificare, se actualizează lungimea n și se inițializează matricea decoded_path cu n linii și 2 coloane, unde fiecare linie va conține coordonatele unei poziții din labirint. Pentru fiecare element din vectorul path, se realizează decodificarea din indexul unic în coordonate carteziene folosind formule matematice: linia se calculează ca fiind partea întreagă a împărțirii (codif - 1) / cols + 1, iar coloana se calculează ca (codif - 1 - (linie - 1) * cols) + 1. Aceste formule reprezintă inversa transformării utilizate în funcția get_adjacency_matrix pentru a calcula indexul unic al unei poziții.

Taskul 2: Linear-Regression:
parse_data_set_file.m:
	Se deschide fișierul specificat prin parametrul file_path cu permisiuni de citire text (rt) și se verifică dacă operațiunea a reușit. Din prima linie a fișierului se extrag două numere întregi reprezentând numărul de linii (m) și numărul de coloane (n) ale setului de date. Se inițializează vectorul țintă Y cu m elemente și matricea InitialMatrix cu m linii și n-1 (prima coloană, cea de prețuri fiind reținută de Y) coloane pentru stocarea caracteristicilor. Pentru fiecare dintre cele m linii din fișier, funcția citește textul cu funcția fgets(), elimină caracterul newline rămas și împarte linia în cuvinte cu funcția strsplit(). Se efectuează unirea pentru termenii compuși cum ar fi "semi-furnished" sau "unfurnished", care sunt separați în urma împărțirii inițiale. Prima valoare de pe fiecare linie este convertită la număr folosind funcția str2double (care convertește un șir de caractere în număr) și stocată în vectorul Y. Celelalte valori sunt procesate și plasate în matricea InitialMatrix, efectuându-se automat conversia la număr acolo unde este posibil, altfel păstrându-se ca șiruri de caractere. Verificarea dacă convertirea a reușit se face folosind funcția isnumeric() și nr==nr (NaN (Not-a-Number) este un tip special de valoare care nu este egală cu nici o altă valoare, inclusiv cu ea însăși. Astfel, nr==nr va returna false dacă nr este NaN.).După prelucrarea tuturor liniilor, fișierul este închis. Despre funcțiile utilizate m-am documentat din https://octave.sourceforge.io/octave/function/strsplit.html, https://octave.sourceforge.io/octave/function/ostrsplit.html, https://octave.sourceforge.io/octave/function/fgetl.html, https://octave.sourceforge.io/octave/function/isnumeric.html, https://octave.sourceforge.io/octave/function/str2double.html.

prepare_for_regression.m:
	În n și m se rețin numărul de linii/coloane ale matricei inițiale, iar matricea de ieșire este inițializată cu n linii și m+1 coloane pentru a ține cont de variabilele care necesită codificare one-hot de la secțiunea teoretică în care se prezintă funcția. Funcția parcurge fiecare element al matricei inițiale utilizând două bucle for. Dacă elementul este numeric, acesta este copiat direct în FeatureMatrix. Dacă elementul este un șir de caractere, se verifică tipul său folosind strcmp. Pentru valorile de tip yes/no, acestea sunt actualizate la 1 și 0. Pentru tipurile de mobilier (furnished, unfurnished, semi-furnished), se aplică codificarea one-hot, adăugând două coloane suplimentare pentru fiecare categorie și valorile respective cazului.

linear_regression_cost_function.m:
	În n se reține numărul de linii din vectorul Y. Apoi, adaugă un termen liber (coloană de 1) la matricea FeatureMatrix, necesar pentru a include bias-ul în modelul de regresie. Erorile sunt calculate ca diferența dintre valorile reale din Y și valorile prezise de model (FeatureMatrix * Theta). Aceste erori sunt ridicate la pătrat și însumate folosind funcția sum(), iar funcția de cost finală este determinată conform formulei din secțiunea teoretică 2.1.

parse_csv_file.m:
	Se deschide fișierul specificat prin parametrul file_path cu permisiuni de citire text (rt) și se verifică dacă operațiunea a reușit. Prima linie a fișierului, care conține antetele coloanelor, este citită cu fgets și separată în componente individuale folosind strsplit, pe baza delimitatorului ,. Pentru citirea datelor efective, funcția utilizează textscan(), specificând un format dinamic utilizând funcția repmat() pentru a se adapta numărul de coloane. Vectorul Y este inițializat cu dimensiunea corespunzătoare primei coloane, iar matricea InitialMatrix reține restul datelor. Procesarea datelor se face în două etape: mai întâi, prima coloană este convertită în valori numerice și stocată în Y folosind str2double(). Apoi, restul coloanelor sunt parcurse și fiecare valoare este verificată cu str2double pentru a determina dacă poate fi convertită într-un număr. Valorile numerice sunt stocate direct în InitialMatrix, iar cele care nu pot fi convertite rămân ca șiruri de caractere. Principiul este asemănător ca la funcția parse_data_set_file. La final, fișierul este închis cu fclose. În această funcție am folosit diverse funcții despre care m-am documentat din https://octave.sourceforge.io/octave/function/repmat.html, https://docs.octave.org/doxygen/8/de/d97/classtextscan.html. Am ales să folosesc textscan() în loc de funcții specializate parsării pe fișiere tip csv cum era recomandat pentru că aceasta oferă mai multă flexibilitate.

gradient_descent.m:
	Se inițializează vectorul Theta cu valori zero, având n+1 linii, incluzând și termenul de bias. În cadrul unei bucle for care rulează pentru iter pași, se calculează erorile dintre predicțiile modelului (FeatureMatrix * Theta(2:n + 1)) și valorile reale din Y. Apoi se calculează gradientul utilizându-se de eroare conform formulei specifice. Vectorul Theta este actualizat la fiecare pas folosind regula de actualizare, unde alpha este rata de învățare. Funcția reprezintă implementarea formulelor explicate la secțiunea teoretică 2.1.1.

normal_equation.m:
	În n și m se rețin numărul de linii/coloane din matricea de caracteristici și se inițializează vectorul coloană Theta cu m+1 (pentru a conține termenul de bias) linii. Se calculează matricea sistemului ( A=X^T*X ) și a vectorului termenilor liberi (b= X^T*Y), unde X este matricea de caracteristici și Y este vectorul țintă. Se verifică dacă matricea A este pozitiv definită, o condiție necesară pentru aplicarea metodei gradientului conjugat. Aceasta se face prin verificarea criteriului lui Sylvester. Dacă matricea A este pozitiv definită, funcția inițializează vectorul soluție Theta și rezidualul inițial ( r=b-A*x_0 ), unde x_0 este vectorul inițializat la început cu zero. Algoritmul este aplicat iterativ, actualizând soluția Theta, rezidualul r, și vectorul conjugat v la fiecare pas. Bucla se oprește până când continuă norma rezidualului scade sub toleranța specificată (pătratul toleranței tol_squared) sau se atinge numărul maxim de iterații. La final, vectorul Theta este ajustat pentru a exclude termenul de bias. Funcția implementează pseudocodul și formulele prezentate la secțiunea teoretică 2.1.1.

lasso_regression_cost_function.m:
	În n se reține numărul de linii/exemple din vectorul Y. Apoi, se adaugă o coloană de 1 la matricea FeatureMatrix pentru a include termenul de bias în calcul. Erorile dintre valorile reale din Y și predicțiile modelului (FeatureMatrix * Theta) sunt ridicate la pătrat și însumate folosind sum(). Funcția de cost este calculată conform formulei din teorie, unde primul termen reprezintă eroarea pătratică medie, iar al doilea termen este regularizarea L1, calculată cu norm(Theta, 1), care reprezintă suma valorilor absolute ale coeficienților din Theta. Această funcție folosește formulele din secțiunea teoretică 2.1.2.

ridge_regression_cost_function.m:
	În n se reține numărul de linii/exemple din vectorul Y. Apoi, adaugă o coloană de 1 la matricea FeatureMatrix pentru a include termenul de bias în calcul. Erorile dintre valorile reale din Y și predicțiile modelului (FeatureMatrix * Theta) sunt ridicate la pătrat și însumate folosind sum. Funcția de cost este calculată conform formulei din teorie, unde primul termen reprezintă eroarea pătratică medie, iar al doilea termen este regularizarea L2, calculată ca suma pătratelor coeficienților din Theta, asemănător ca la funcția anterioară. Această funcție folosește formulele din secțiunea teoretică 2.1.2.

Taskul 3: MNIST-101:
load_dataset.m:
	Se folosește funcția load() pentru a încărca conținutul fișierului în variabila info. După încărcare, funcția verifică dacă datele încărcate sunt de tip structură folosind isstruct(). Dacă verificarea este validă, funcția extrage matricea de caracteristici X și vectorul țintă y din câmpurile structurii info. Altfel, funcția se oprește. În această implementare m-am documentat despre funțiile folosite din https://octave.sourceforge.io/octave/function/isstruct.html, https://octave.sourceforge.io/octave/function/load.html.

split_dataset.m:
	Se reține în n numărul de linii/exemple din matricea X. Apoi, creează un vector de indici aleatori folosind rand() pentru generarea numerelor aleatoare și sort() pentru a obține indici în ordine crescătoare. Numărul de exemple pentru antrenare este calculat ca fiind rotunjirea (round()) produsului dintre numărul total de exemple și procentul percent utilizând round. În X_train și y_train se memorează primele nr_train exemple selectate pe baza indicilor aleatori, iar în X_test și y_test conțin restul exemplelor. Funcția se bazează pe explicația cerinței funcției și pe explicația secțiunii teoretice 3.2.4.

initialize_weights.m:
	Se calculează o valoare eps_0 folosind formula explicată în teorie.
Această valoare este utilizată pentru a genera valori aleatorii din intervalul (-eps_0, eps_0), folosind funcția rand() pentru generarea numerelor aleatoare uniforme. Matricea rezultată are L_{next} linii și L_{prev} + 1 coloane, unde coloana suplimentară este pentru termenul de bias. Această funcție este implementată pe baza explicațiilor din secțiunea 3.2.6. Formula pentru generarea unui număr random dintr-un anumit interval se bazează pe formula explicată aici https://www.mathworks.com/help/matlab/math/floating-point-numbers-within-specific-range.html.

cost_function.m:
	În n se reține numărul de linii/exemple din vectorul X. Funcția reface matricile de greutăți th1 și th2 utilizând reshape(), pe baza dimensiunilor stratului de intrare, stratului ascuns și stratului de ieșire. Vectorul de etichete y este transformat într-o matrice binară matr_y, unde fiecare rând reprezintă eticheta unui exemplu sub forma unui vector one-hot. În etapa de forward propagation, activările pentru stratul ascuns și stratul de ieșire sunt calculate folosind transformări liniare și funcția sigmoid. Stratului de intrare a1 și stratului ascuns a2 li se adaugă termenul de bias. Activările stratului de ieșire a3 sunt utilizate pentru a calcula funcția de cost J care măsoară diferența dintre predicțiile modelului și etichetele reale. Regularizarea este adăugată la funcția de cost prin calcularea normei L2 a greutăților din th1 și th2, excluzând termenii de bias. În etapa de backpropagation, erorile pentru stratul de ieșire del3 și stratul ascuns del2 sunt calculate. Derivata funcției sigmoid este utilizată pentru a calcula del2, iar dimensiunile sunt ajustate pentru a exclude termenul de bias. Gradientul pentru fiecare strat este determinat prin produsele dintre erori și activări, rezultând matricile grad1 și grad2. La final, gradientul este normalizat și vectorizat în variabila grad. În implementarea acestei funcției au fost folosite noțiunile teoretice și formulele din secțiunile 3.2.4 și 3.2.5. De asemenea, am utilizat funcția recomandată reshape() despre care m-am documentat din https://octave.sourceforge.io/communications/function/reshape.html.

predict_classes.m:
	În n se reține numărul de linii/exemple din vectorul X. Greutățile rețelei sunt reconstruite din vectorul weights în două matrice, th1 și th2, utilizând funcția reshape() , pe baza dimensiunilor stratului de intrare, stratului ascuns și stratului de ieșire. În etapa de forward propagation, activările stratului de intrare a1 sunt extinse prin adăugarea unui termen de bias (coloană de 1). Activările stratului ascuns a2 sunt calculate prin aplicarea funcției sigmoid pe produsul dintre th1 și a1. Stratului ascuns i se adaugă, de asemenea, un termen de bias. Activările stratului de ieșire a3 sunt calculate prin aplicarea funcției sigmoid pe produsul dintre th2 și a2. Pentru fiecare exemplu/linie, clasa este determinată ca fiind indexul valorii maxime din vectorul de activări a3, utilizând funcția max(). Rezultatele sunt memorate în vectorul classes, care conține toate clasele prezise. În implementarea acestei funcții am folosit secțiunea teoretică 3.2.4.